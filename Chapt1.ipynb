{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.NLP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Things In NLP\n",
    "1. Tokenization\n",
    "2. Stemming And Lemmatization\n",
    "3. Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i want to go outside .', 'but the weather is not clear.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence Tokenization -  Splitting sentences in the paragraph\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"i want to go outside . but the weather is not clear.\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'want',\n",
       " 'to',\n",
       " 'go',\n",
       " 'outside',\n",
       " '.',\n",
       " 'but',\n",
       " 'the',\n",
       " 'weather',\n",
       " 'is',\n",
       " 'not',\n",
       " 'clear',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Tokenization – Splitting words in a sentence.\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming  And Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming \n",
    "#1. Porter’s Stemmer algorithm\n",
    "# 2. Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program program\n",
      "programs program\n",
      "programer program\n",
      "programing program\n",
      "programers program\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "ps = PorterStemmer()\n",
    "words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"]\n",
    "for i in words:\n",
    "    print(i,ps.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program program\n",
      "programs program\n",
      "programer programer\n",
      "programing programing\n",
      "programers programers\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "ws = WordNetLemmatizer()\n",
    "for i in words:\n",
    "    print(i,ws.lemmatize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem\n",
      "Programers program\n",
      "program program\n",
      "with with\n",
      "programing program\n",
      "languages languag\n",
      "\n",
      "\n",
      "Programers Programers\n",
      "program program\n",
      "with with\n",
      "programing programing\n",
      "languages language\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize #step1\n",
    "sentence = \"Programers program with programing languages\"\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "#stemming\n",
    "print('stem')\n",
    "for i in words:\n",
    "    print(i,ps.stem(i))\n",
    "\n",
    "print(\"\\n\")\n",
    "for j in words:\n",
    "    print(j,ws.lemmatize(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Programers', 'program', 'with', 'programing', 'languages']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy \n",
    "gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These Are All Stops Word That We Have To Remove\n",
      "\n",
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(\"These Are All Stops Word That We Have To Remove\\n\")\n",
    "print(len(stopwords.words('english')))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sent = \"\"\"This is a sample sentence,\n",
    "                        showing off the stop words filtration.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(example_sent)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_sentence = [w for w in words if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All put in together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Thank you all so very much. Thank you to the Academy.\n",
    "Thank you to all of you in this room. I have to congratulate\n",
    "the other incredible nominees this year. The Revenant was\n",
    "the product of the tireless efforts of an unbelievable cast\n",
    "and crew. First off, to my brother in this endeavor, Mr. Tom\n",
    "Hardy. Tom, your talent on screen can only be surpassed by\n",
    "your friendship off screen … thank you for creating a t\n",
    "ranscendent cinematic experience. Thank you to everybody at\n",
    "Fox and New Regency … my entire team. I have to thank\n",
    "everyone from the very onset of my career … To my parents;\n",
    "none of this would be possible without you. And to my\n",
    "friends, I love you dearly; you know who you are. And lastly,\n",
    "I just want to say this: Making The Revenant was about\n",
    "man's relationship to the natural world. A world that we\n",
    "collectively felt in 2015 as the hottest year in recorded\n",
    "history. Our production needed to move to the southern\n",
    "tip of this planet just to be able to find snow. Climate\n",
    "change is real, it is happening right now. It is the most\n",
    "urgent threat facing our entire species, and we need to work\n",
    "collectively together and stop procrastinating. We need to\n",
    "support leaders around the world who do not speak for the\n",
    "big polluters, but who speak for all of humanity, for the\n",
    "indigenous people of the world, for the billions and\n",
    "billions of underprivileged people out there who would be\n",
    "most affected by this. For our children’s children, and\n",
    "for those people out there whose voices have been drowned\n",
    "out by the politics of greed. I thank you all for this\n",
    "amazing award tonight. Let us not take this planet for\n",
    "granted. I do not take tonight for granted. Thank you so very much.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords #stopword\n",
    "from nltk.stem.porter import PorterStemmer #for Stemming\n",
    "from nltk.stem import WordNetLemmatizer #for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer() #inialize stemmer\n",
    "wordnet=WordNetLemmatizer() #inialize lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step- 1\n",
    "sentences = nltk.sent_tokenize(paragraph) #sentence Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    # step 2 using some preprocess like remove unnecessary word,lower the sentence etc\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    #step3 use stemmer and remove stopwords in sentence\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    #step3 use lemmatize and remove stopwords in sentence Use any one of these.\n",
    "#     review = [wordnet.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank much',\n",
       " 'thank academi',\n",
       " 'thank room',\n",
       " 'congratul incred nomine year',\n",
       " 'reven product tireless effort unbeliev cast crew',\n",
       " 'first brother endeavor mr tom hardi',\n",
       " 'tom talent screen surpass friendship screen thank creat ranscend cinemat experi',\n",
       " 'thank everybodi fox new regenc entir team',\n",
       " 'thank everyon onset career parent none would possibl without',\n",
       " 'friend love dearli know',\n",
       " 'lastli want say make reven man relationship natur world',\n",
       " 'world collect felt hottest year record histori',\n",
       " 'product need move southern tip planet abl find snow',\n",
       " 'climat chang real happen right',\n",
       " 'urgent threat face entir speci need work collect togeth stop procrastin',\n",
       " 'need support leader around world speak big pollut speak human indigen peopl world billion billion underprivileg peopl would affect',\n",
       " 'children children peopl whose voic drown polit greed',\n",
       " 'thank amaz award tonight',\n",
       " 'let us take planet grant',\n",
       " 'take tonight grant',\n",
       " 'thank much']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
